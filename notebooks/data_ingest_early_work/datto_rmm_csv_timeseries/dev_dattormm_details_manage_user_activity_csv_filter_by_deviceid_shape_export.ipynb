{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6a9eacd1c39cce",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Datto RMM - CSV import - Activity Log - Shape - Export to PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30913ebb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import PostgreSQL Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df2062",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import psycopg2 as pg\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Import the 'config'.py file\n",
    "import config.config as config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07be08d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import DataFrame Import and Shaping Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0259f6d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import modules.field_standards as fs\n",
    "from modules.source_file_info import AddTime as srctime\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a961e91",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Arguments and Declarations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898006d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define key column to join on\n",
    "fieldnames_to_compare = 'Device UID'\n",
    "\n",
    "# identify folder stages so that files are not called twice in the same stage\n",
    "source_dir = 'd:/data_sets/data_pool/datto_rmm_data/audit_logs/'\n",
    "# source_dir = 'D:\\cloud_storage\\OneDrive - Think Stack\\Reports Automation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383724b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define if you are pulling raw data ('data_pool') or test data ('seed_data')\n",
    "database = 'seed_data'\n",
    "\n",
    "# exclude csv if missing column name\n",
    "exclude_on_missing_column = False\n",
    "\n",
    "# DEFINE THE DB URI\n",
    "db_uri = f'postgresql://{config.USERNAME}:{config.SECRET}@{config.URI}:{config.PORT}/{database}'\n",
    "\n",
    "# DEFINE THE ENGINE (CONNECTION OBJECT)\n",
    "engine = create_engine(db_uri, echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1490724",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dictionary of df_names and queries used to create tables in postreSQL\n",
    "device_details = {'dataframe': 'device_details', 'query': 'SELECT * FROM datto_rmm.device_details'}\n",
    "os_patch_mgmt = {'dataframe': 'os_patch_mgmt', 'query': 'SELECT * FROM datto_rmm.os_patch_mgmt'}\n",
    "user_activity = {'dataframe': 'audit_logs', 'query': 'SELECT * FROM datto_rmm.audit_logs'}\n",
    "\n",
    "import_dataframes = [audit_logs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610212da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selected Columns from Master Device View Export CSV - This is the column mask that will be used to trimp the outer merge on match key column\n",
    "\n",
    "device = [\n",
    "    'Device UID',\n",
    "    'Device Hostname',\n",
    "    'Site UID',\n",
    "    'Site Name',\n",
    "    'Device Description',\n",
    "    'Int IP Address',\n",
    "    'Ext IP Addr',\n",
    "    'Create Date',\n",
    "    'Last Seen',\n",
    "    'Last Audit Date',\n",
    "    'Session Name',\n",
    "    'Privacy Mode',\n",
    "    'Agent Version',\n",
    "    'Display Version',\n",
    "    'Device Model',\n",
    "    'Operating System',\n",
    "    'Serial Number',\n",
    "    'Motherboard',\n",
    "    'Device CPU',\n",
    "    'Physical CPU Cores',\n",
    "    '.NET Version',\n",
    "    'Memory',\n",
    "    'MAC Address(es)',\n",
    "    'User-Defined Field 10',\n",
    "    'Device Type',\n",
    "    'Domain',\n",
    "    'Disk Drive (total/free)',\n",
    "    'Online Duration (hrs)',\n",
    "    'Architecture',\n",
    "    'Display Adapters',\n",
    "    'BIOS Name',\n",
    "    'BIOS Release Date',\n",
    "    'BIOS Version',\n",
    "    'Last Reboot',\n",
    "    'Reboot required',\n",
    "    'NIC Vendor',\n",
    "    'Manufacturer',\n",
    "]\n",
    "\n",
    "manage = [\n",
    "    'Device UID',\n",
    "    'Site Name',\n",
    "    'Device Hostname',\n",
    "    'Device Description',\n",
    "    'Policy',\n",
    "    'Int IP Address',\n",
    "    'Ext IP Addr',\n",
    "    'Last User',\n",
    "    'Group',\n",
    "    'Create Date',\n",
    "    'Last Seen',\n",
    "    'Last Audit Date',\n",
    "    'Session Name',\n",
    "    'Agent Version',\n",
    "    'Operating System',\n",
    "    'Service Pack',\n",
    "    'Serial Number',\n",
    "    'User-Defined Field 10',\n",
    "    'Last Run',\n",
    "    'Schedule',\n",
    "    'Patch Status',\n",
    "    'Patches Approved Pending',\n",
    "    'Patches Installed',\n",
    "    'Patches Not Approved',\n",
    "    'Device Type',\n",
    "    'Domain',\n",
    "    'Disk Drive (total/free)',\n",
    "    'Online Duration (hrs)',\n",
    "    'Last Reboot',\n",
    "    'Reboot required',\n",
    "    'Manufacturer'\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddda615",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# output of csv with matching key column\n",
    "included_files = {}\n",
    "\n",
    "# output csv of all files that could not be merged\n",
    "excluded_files = {}\n",
    "\n",
    "# Regex Match to group files to be combined on rows rather than merged on columns to prevent dropping rows if there isnt a key column match when files are combined in random order\n",
    "pattern = re.compile(r'^(\\w+)_')\n",
    "\n",
    "# CSV File Types\n",
    "devices_tab_export_filename = 'DeviceDetailsExport'\n",
    "manage_tab_export_filename = 'SystemDeviceSelection'\n",
    "user_activity_export_filename = 'UserActivity'\n",
    "grouped_export_files_list = []\n",
    "shaped_df_object_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c4198",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Read all files in source_dir and sub directories</h1>\n",
    "    <h3> Filter by '.csv' </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af186e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pull all filenames walking through all folders (recursive going down the tree)\n",
    "source_csv_dict = {}\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    for file in files:\n",
    "        if '.csv' in file:\n",
    "            source_csv_dict.update({os.path.join(file): os.path.join(root, file)})\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9774255",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "# print(all_source_csv)\n",
    "print('All CSV Files found before futher vetting and filtering')\n",
    "print('='*50)\n",
    "for k,v in source_csv_dict.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129aa9c1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Sorting and Excluding Files</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da86f51c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read all csv file columns and create two lists of files:\n",
    "### Those with the chosen merge key column will be kept and the remaining filenames will not be called any further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe29a3b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for k, v in source_csv_dict.items():\n",
    "    if exclude_on_missing_column:\n",
    "        df = pd.read_csv(v)\n",
    "        if fieldnames_to_compare not in df.columns:\n",
    "            print(f'Missing Key: {fieldnames_to_compare} to Join in {k}')\n",
    "            excluded_files.update({k: v})\n",
    "        else:\n",
    "            included_files.update({k: v})\n",
    "    else:\n",
    "        included_files.update({k: v})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "338560bf",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    },
    "tags": []
   },
   "source": [
    "print('Files with CORRECT join key column:')\n",
    "print('-'*50)\n",
    "for file in included_files:\n",
    "    print(file)\n",
    "print('='*50)\n",
    "\n",
    "print('Files MISSING join key column:')\n",
    "print('-'*50)\n",
    "for file in excluded_files:\n",
    "    print(file)\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47755523",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parse Accepted CSV's for file discription and store as dictionary key pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2598632",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'^([a-zA-Z]{0,})(\\_|\\-|''){0,1}([a-zA-Z]{0,})')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "628d544f",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "for k,v in included_files.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d946c2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k, v in included_files.items():\n",
    "    matches = pattern.search(k)\n",
    "    if matches[1] == devices_tab_export_filename:\n",
    "        #print(f'''['{v}'] matches: ['{devices_tab_export_filename}'] on ['{matches[1]}']''')\n",
    "        grouped_export_files_list.append({'filename': v, 'groupname': 'device_details', 'columns': device})\n",
    "    elif matches[3] == manage_tab_export_filename:\n",
    "        #print(f'''['{v}'] matches: ['{manage_tab_export_filename}'] on ['{matches[3]}']''')\n",
    "        grouped_export_files_list.append({'filename': v, 'groupname': 'os_patch_mgmt', 'columns': manage})\n",
    "    elif matches[1] == audit_log_export_filename:\n",
    "        print(f'''['{v}'] matches: ['{audit_log_export_filename}'] on ['{matches[1]}']''')\n",
    "        grouped_export_files_list.append({'filename': v, 'groupname': 'user_activity', 'columns': device})\n",
    "\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3213df9b",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "for k in grouped_export_files_list:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ec682",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## More Shaping on User Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c5ae90",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for file in grouped_export_files_list:\n",
    "    filename = file['filename']\n",
    "    groupname = file['groupname']\n",
    "    grp_columns = file['columns']\n",
    "    print(filename)\n",
    "    print(groupname)\n",
    "    print(grp_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979a745",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Shape CSV for to add or trim to bring columns to standard\n",
    "\n",
    "with open(filename, 'w+', newline='') as file_shaping:\n",
    "    csv_reader = csv.DictReader(file_shaping)\n",
    "    # csv_writer writes the following nested code to the shaped_dir file (unmerged / not-joined)\n",
    "    csv_writer = csv.DictWriter(file_shaping, fieldnames=grp_columns, extrasaction='ignore', delimiter=',')\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "#for each line read as dict add (3) source data dict k,v elements with the source data then complete the row\n",
    "for line in csv_reader:\n",
    "    csv_writer.writerow(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba724f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## For those files that have the key column, shape add source info\n",
    "\n",
    "### 1. Add in any missing columns against the standard so data columns line up on import\n",
    "### 3. Trim extra df columns to match column standards\n",
    "### 4. Replace any known type mismatch values before setting datetime\n",
    "### 5. Add source file data as columns at end of dataframe (record the file creation, modified, and fullpath name)\n",
    "### 6. Parse known date columns to datetime so the types are correct in db import\n",
    "### 7. EXPORT to postregsql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c4c7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in grouped_export_files_list:\n",
    "    filename = file['filename']\n",
    "    groupname = file['groupname']\n",
    "    grp_columns = file['columns']\n",
    "    print(filename)\n",
    "\n",
    "    # pull source time from file properties\n",
    "    source_info = srctime(filename)\n",
    "\n",
    "    # NA Values Check\n",
    "    if groupname == 'UserActivity':\n",
    "        na_values = ['null', '(null)']\n",
    "    else:\n",
    "        na_values = ['Currently Online', 'null', '(null)']\n",
    "\n",
    "        # Import CSV to Pandas\n",
    "    print(f\"reading file ['{filename}'] and ['{groupname}']!\")\n",
    "    df = pd.read_csv(filename, na_values=na_values, skipinitialspace=True)\n",
    "\n",
    "    # if column is missing in dataframe add it before upload to prevent mismatch or multi indexed columns\n",
    "    for c in grp_columns:\n",
    "        if c not in df:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    # DTYPES\n",
    "\n",
    "    # prepare DTYPE values to match predicted values - get column initial dtypes\n",
    "    column_dtypes = dict(df.dtypes)\n",
    "\n",
    "    for k, v in column_dtypes.items():\n",
    "\n",
    "        # strip any whitespace from object columns (non-datetime or boolean)\n",
    "        if v == 'object':\n",
    "            df[k].str.strip()\n",
    "\n",
    "        # Condition boolean values for postgreSQL: (True,False,NULL) Only!\n",
    "        elif v == 'bool':\n",
    "            df[k].mask(df[k] == '', pd.NA, inplace=True)\n",
    "\n",
    "    # regex remove whitespace\n",
    "    df = df.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "    # Replacment Values for type mismatch\n",
    "    replace_dict = {\n",
    "        'Currently Online': source_info['Source Modified Date'],\n",
    "        '': pd.NA\n",
    "    }\n",
    "    df.replace(replace_dict)\n",
    "\n",
    "    # drop non-standard columns\n",
    "    df.drop([col for col in df.columns if col not in grp_columns], axis=1, inplace=True)\n",
    "    #print(f'columns after drop: {df.columns}')\n",
    "\n",
    "    # add source info to new columns k with values v\n",
    "    for k, v in source_info.items():\n",
    "        df[k] = v\n",
    "\n",
    "    # Parse Date Data Options\n",
    "    date_parser = lambda c: pd.to_datetime(c, errors='coerce')\n",
    "\n",
    "    details_manage__dates = [\n",
    "        'Create Date',\n",
    "        'Last Seen',\n",
    "        'Last Reboot',\n",
    "        'Source Creation Date',\n",
    "        'Source Modified Date'\n",
    "    ]\n",
    "\n",
    "    audit_log_dates = [\n",
    "        'Date/Time'\n",
    "    ]\n",
    "\n",
    "    # choose parse by groupname\n",
    "    if groupname == 'UserActivity':\n",
    "        parse_dates = audit_log_dates\n",
    "    else:\n",
    "        parse_dates = details_manage__dates\n",
    "\n",
    "    # filename prefix timestamp format\n",
    "    time_format = '%Y_%m_%d_%H%M%S'\n",
    "\n",
    "    # convert dates to datetime\n",
    "    df[parse_dates] = df[parse_dates].apply(date_parser)\n",
    "\n",
    "    # section can be uncommented for seed data creation for import on a new table to set column names and types\n",
    "    df.to_csv('.csv')\n",
    "\n",
    "    # export to postgresql\n",
    "    #df.to_sql(groupname, con=engine, if_exists='append', index=False, index_label=None, schema='datto_rmm')\n",
    "    print('=' * 100)\n",
    "    print('')\n",
    "    print('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0fa198",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45917f99",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
