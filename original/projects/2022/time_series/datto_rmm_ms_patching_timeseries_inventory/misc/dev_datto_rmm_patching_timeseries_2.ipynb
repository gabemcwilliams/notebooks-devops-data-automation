{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1> DattoRMM - MS Patching Timeseries </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.offline as offline\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import xlrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Arguments and Declarations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Selected Columns from Master Device View Export CSV - This is the column mask that will be used to trimp the outer merge on match key column\n",
    "std_columns = [\n",
    "    'Device UID',\n",
    "    'Site Name',\n",
    "    'Site UID',\n",
    "    'Device Hostname',\n",
    "    'Create Date',\n",
    "    'Last Seen',\n",
    "    'Last Audit Date',\n",
    "    'Policy',\n",
    "    'Patches Approved Pending',\n",
    "    'Patches Not Approved',\n",
    "    'Patches Installed',\n",
    "    'Patch Status',\n",
    "    'Schedule',\n",
    "    'Last Run',\n",
    "    'Operating System',\n",
    "    'Device CPU',\n",
    "    'Physical CPU Cores',\n",
    "    '.NET Version',\n",
    "    'Memory',\n",
    "    'Device Type',\n",
    "    'Domain',\n",
    "    'Disk Drive (total/free)',\n",
    "    'Online Duration (hrs)',\n",
    "    'Architecture',\n",
    "    'Last Reboot',\n",
    "    'Reboot required',\n",
    "    'Int IP Address',\n",
    "    'User-Defined Field 10',\n",
    "    'MAC Address(es)',\n",
    "    'Software Status',\n",
    "    'Group',\n",
    "    'Antivirus Product',\n",
    "    'Antivirus Status',\n",
    "    'Source Modified Date',\n",
    "    'Source Creation Date',\n",
    "    'Source Filename'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# review column filter for any duplicates values or close similarities\n",
    "std_col_ser = pd.Series(std_columns).value_counts()\n",
    "std_col_ser[std_col_ser > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# filename prefix timestamp format\n",
    "time_format = '%Y_%m_%d_%H%M%S'\n",
    "\n",
    "# define key column to join on\n",
    "fieldnames_to_compare = 'Device UID'\n",
    "\n",
    "# identify folder stages so that files are not called twice in the same stage\n",
    "source_dir = 'D:/cloud_storage/Think Stack/Infrastructure - Documents/Reports - Archiving and Distribution'\n",
    "\n",
    "# Parse Date Data Options\n",
    "date_parser = lambda c: pd.to_datetime(c, errors='coerce')\n",
    "parse_dates =  ['Create Date', 'Last Seen','Last Reboot']\n",
    "\n",
    "# NA Values Check\n",
    "na_values = ['Currently Online','null', '(null)']\n",
    "\n",
    "# output of csv with matching key column\n",
    "included_files = {}\n",
    "\n",
    "# output csv of all files that could not be merged\n",
    "excluded_files = {}\n",
    "\n",
    "# Regex Match to group files to be combined on rows rather than merged on columns to prevent dropping rows if there isnt a key column match when files are combined in random order\n",
    "pattern = re.compile(r'^(\\w+)_')\n",
    "\n",
    "# final dataframe before training and visualization\n",
    "df_clean = pd.DataFrame(columns=std_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Read all files in source_dir and sub directories</h1>\n",
    "    <h3> Filter by '.csv' </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pull all filenames walking through all folders (recursive going down the tree)\n",
    "#all_source_csv = []\n",
    "source_report_dict = {}\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    for file in files:\n",
    "        if ('.csv' in file) | ('.xlsx' in file):\n",
    "            #all_source_csv.append(os.path.join(root,file))\n",
    "            source_report_dict.update({os.path.join(file):os.path.join(root,file)})\n",
    "\n",
    "# print(all_source_csv)\n",
    "print('All CSV Files found before futher vetting and filtering')\n",
    "print('='*50)\n",
    "for file in source_report_dict:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Sorting and Excluding Files</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read all csv file columns and create two lists of files:\n",
    "### Those with the chosen merge key column will be kept and the remaining filenames will not be called any further"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "filename = ''\n",
    "\n",
    "for k,v in source_csv_dict.items():\n",
    "    df = pd.read_csv(v)\n",
    "    #print(df['Site Name'].unique())\n",
    "    #compare_keys(df,filename)\n",
    "    if fieldnames_to_compare not in df.columns:\n",
    "        print(f'Missing Key: {fieldnames_to_compare} to Join in {filename}')\n",
    "        excluded_files.update({k:v})\n",
    "    else:\n",
    "        included_files.update({k:v})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "print('Files with CORRECT join key column:')\n",
    "print('-'*50)\n",
    "for file in included_files:\n",
    "    print(file)\n",
    "print('='*50)\n",
    "\n",
    "print('Files MISSING join key column:')\n",
    "print('-'*50)\n",
    "for file in excluded_files:\n",
    "    print(file)\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parse Accepted Reports for file discription and store as dictionary key pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "string = 'ExampleFCU - 2020 - 11 - Patching Audit.xlsx'\n",
    "ms_patch_prog = re.compile(r'(\\bSIG\\b|\\bExampleFCU\\b|\\bExample FCU\\b){1}.*(?!\\b3rd\\b)+(\\bMS\\b|\\bPatch\\b|\\bPatching\\b)+')\n",
    "third_prog = re.compile(r'(\\b3rd\\b|\\b3PP\\b)+')\n",
    "extension_prog = re.compile(r'(\\.\\w{3,4})')\n",
    "report_date_prog = re.compile(r'(\\d{4}).*(\\d{2})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ms_patch_reports = []\n",
    "for k,v in source_report_dict.items():\n",
    "    match = {}\n",
    "\n",
    "    if (ms_patch_prog.search(k) != None) & (third_prog.search(k) == None):\n",
    "\n",
    "        # date nested dict\n",
    "        date = {}\n",
    "        date['year'] = str(report_date_prog.findall(k)[0][0])\n",
    "        date['month'] = str(report_date_prog.findall(k)[0][1])\n",
    "\n",
    "        match['filename'] = k\n",
    "        match['path'] = v\n",
    "\n",
    "        # extension\n",
    "        if extension_prog.findall(file['filename']) == ['.xlsx']:\n",
    "            match['extension'] = 'xlsx'\n",
    "\n",
    "        elif extension_prog.findall(file['filename']) == ['.csv']:\n",
    "            match['extension'] = 'csv'\n",
    "\n",
    "        match['date'] = date\n",
    "\n",
    "        ms_patch_reports.append(match)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "# Export the originals (will delete timestamp)\n",
    "import shutil\n",
    "for file in ms_patch_reports:\n",
    "    shutil.copyfile(file['path'], f\"d:/exports/{file['filename']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## For those files that have the key column, set index col and add source info\n",
    "### 1. Add source file data as columns at end of dataframe (record the file creation, modified, and fullpath name)\n",
    "### 2. Set index col = fieldnames_to_compare variable list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "def AddTime(source_file):\n",
    "    source_mfdate = 'Source Modified Date'\n",
    "    source_crdate = 'Source Creation Date'\n",
    "    source_fn = 'Source Filename'\n",
    "\n",
    "    # Both the variables would contain time\n",
    "    # elapsed since EPOCH in float\n",
    "    ti_c = os.path.getctime(source_file)\n",
    "    ti_m = os.path.getmtime(source_file)\n",
    "    fi_n = os.path.basename(source_file)\n",
    "\n",
    "\n",
    "    # Converting the time in seconds to UTC datetime\n",
    "    c_ti = datetime.datetime.utcfromtimestamp(ti_c).strftime('%Y/%m/%d %H:%M:%S')\n",
    "    m_ti = datetime.datetime.utcfromtimestamp(ti_m).strftime('%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "\n",
    "    return {source_crdate:c_ti,source_mfdate:m_ti,source_fn:fi_n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def map_source(source_file):\n",
    "    # pull source time from file properties\n",
    "    source_info = AddTime(source_file)\n",
    "\n",
    "    # Import CSV\n",
    "    df = pd.read_csv(source_file,index_col=fieldnames_to_compare)\n",
    "\n",
    "    # add source info to new columns k with values v\n",
    "    for k,v in source_info.items():\n",
    "        df[k] = v\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Attempt 2 at updating data correctly on import.  Attempting to use iloc for each df row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ms_patch_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for df in ms_patch_reports:\n",
    "\n",
    "    print(df['filename'])\n",
    "    print(df['path'])\n",
    "    print(df['date'])\n",
    "    print(df['extension'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df2 = df.dropna(how='all',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create blank DataFrame to fill\n",
    "df_master = pd.DataFrame(columns=std_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": "test_details = pd.read_csv('.csv')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": "test_manage = pd.read_csv('.csv')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for row in df_test1.row:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for file in devices_tab_export_files:\n",
    "    #print(file['filename'])\n",
    "    #print(file['groupname'])\n",
    "    df_devices_list.append(map_source(file['filename']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Attempt 1 at updating data correctly on import.  Still having issues with micromanaging file order on join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print(manage_tab_export_files)\n",
    "#print(devices_tab_export_files)\n",
    "for file in devices_tab_export_files:\n",
    "    #print(file['filename'])\n",
    "    #print(file['groupname'])\n",
    "    df_devices_list.append(map_source(file['filename']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for file in manage_tab_export_files:\n",
    "    #print(file['filename'])\n",
    "    #print(file['groupname'])\n",
    "    df_manage_list.append(map_source(file['filename']))\n",
    "\n",
    "\n",
    "#print(df_manage_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices = pd.concat(df_devices_list,axis=0)\n",
    "df_manage = pd.concat(df_manage_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_manage.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_manage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Join, Concat, and Merge</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Those of the same name should be concatonated by row or stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Merge all dataframes (csv's) into an empty dataframe that contains all columns without data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# as a df (dataframe) must be merged on another, we start with filelist with the first element as the df all will be merged into index[0] in list dtype\n",
    "df_clean = df_list[0]\n",
    "\n",
    "for df_object in df_list[1:]:\n",
    "    # join on key column or columns (original 'set' dtype must be changed to 'list' to fit pandas expected argument for 'merge' method)\n",
    "    df_clean.merge(df_object, on=(fieldnames_to_compare), how= 'outer',suffixes=('', '_drop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicate colummns renamed as '_drop' during parse\n",
    "df_clean.drop([col for col in df_clean.columns if 'drop' in col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Trim any columns not in the column standars list 'columns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(df_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_clean.drop([col for col in df_clean.columns if col not in std_columns], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(df_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# review column filter for any duplicates columns and drop one\n",
    "clean_col = dict(df_clean.columns.value_counts())\n",
    "dup_cols = list({k for (k,v) in clean_col.items() if v > 1})\n",
    "if dup_cols:\n",
    "    df_clean.rename(dup_cols,axis=1,inplace=True)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using lists 'parse_dates' as datetime column targets and 'data_parser' as the datetime function to be applied to each value in target columns along each row that will change value type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert dates to datetime\n",
    "# variable 'time_format' stated at declaration\n",
    "df_clean[parse_dates] = df_clean[parse_dates].apply(date_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Add 'Offline 30 days' and 'Extended Reboot' Columns as datetime delta calculations from day this report is run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fill in NaT / NaN data with 0 time so columns can be converted to datetime and datetime methods can be used\n",
    "df_clean[parse_dates].fillna(pd.Timedelta('0 days'),inplace=True)\n",
    "\n",
    "# Filter - Devices Offline 30 Days\n",
    "df_clean['Offline 30 Days'] = df_clean['Last Seen'] > datetime.datetime.now() - pd.to_timedelta(\"30day\")\n",
    "\n",
    "# Filters - Last Reboot Extended Duration and Online without Reboot Extended Duration\n",
    "df_clean['Last Reboot Extended'] = df_clean['Last Reboot'] > datetime.datetime.now() - pd.to_timedelta(\"30day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Apply heatmap to review any NaN or NaT (null) values before they can be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(data = df_clean.isnull(),yticklabels=False,cbar=False,cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Start ML Trials</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Create MS Patching pairplot DataFrame\n",
    "df_patch_pair = df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convert 'Category' columns into numbers to get value relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_patch_status = pd.get_dummies(df_clean['Patch Status'],prefix='d',prefix_sep='_')\n",
    "df_patch_pair.drop('Patch Status',axis=1,inplace=True)\n",
    "df_patch_pair = pd.concat([df_patch_pair,d_patch_status])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_offline_30days = pd.get_dummies(df_clean['Offline 30 Days'],drop_first=True,prefix='d',prefix_sep='_')\n",
    "d_offline_30days.rename(columns={'d_True':'d_Offline_30 Days'},inplace=True)\n",
    "df_patch_pair.drop('Offline 30 Days',axis=1,inplace=True)\n",
    "df_patch_pair = pd.concat([df_patch_pair,d_offline_30days])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_last_reboot_ext = pd.get_dummies(df_clean['Last Reboot Extended'],drop_first=True,prefix='d',prefix_sep='_')\n",
    "d_last_reboot_ext.rename(columns={'d_True':'d_Last Reboot Extended'},inplace=True)\n",
    "df_patch_pair.drop('Last Reboot Extended',axis=1,inplace=True)\n",
    "df_patch_pair = pd.concat([df_patch_pair,d_last_reboot_ext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_av_status = pd.get_dummies(df_clean['Antivirus Status'],drop_first=True,prefix='d',prefix_sep='_')\n",
    "d_av_status.rename(columns={'d_Running & up-to-date':'d_AV Status Ok'},inplace=True)\n",
    "df_patch_pair.drop('Antivirus Status',axis=1,inplace=True)\n",
    "df_patch_pair = pd.concat([df_patch_pair,d_av_status])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_reboot_required = pd.get_dummies(df_clean['Reboot required'],drop_first=True,prefix='d',prefix_sep='_')\n",
    "d_reboot_required.rename(columns={'d_True':'d_Reboot required'},inplace=True)\n",
    "df_patch_pair.drop('Reboot required',axis=1,inplace=True)\n",
    "df_patch_pair = pd.concat([df_patch_pair,d_reboot_required])\n",
    "d_reboot_required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=df_patch_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for col in df_patch_pair.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(30,15))\n",
    "sns.lineplot(data=df_patch_pair,x='Last Reboot',y='Patches Approved Pending',lw=.5)\n",
    "plt.savefig('.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_patch_pair['Online Duration (hrs)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_patch_pair['Site Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(30,15))\n",
    "sns.barplot(data=df_patch_pair,x='Site Name',y='Online Duration (hrs)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# store filename info as dictionary\n",
    "var_dict = {}\n",
    "i = 0\n",
    "df_list = []\n",
    "df_names =[]\n",
    "\n",
    "\n",
    "# create variable dictionary before import iteration\n",
    "for filename in all_shaped_csv:\n",
    "    var_dict.update({('df' + str(i)): (shaped_dir + filename)})\n",
    "    i = i + 1\n",
    "\n",
    "# for filename = key(k) import into pandas and append resulting dataframe to df list as element\n",
    "for k, v in var_dict.items():\n",
    "    k = pd.read_csv(v)\n",
    "    df_names.append(v)\n",
    "    df_list.append(k)\n",
    "\n",
    "#print(df_names)\n",
    "\n",
    "# as a df (dataframe) must be merged on another, we start with filelist with the first element as the df all will be merged into index[0] in list dtype\n",
    "for df_object in df_list[1:]:\n",
    "    #print(df_object)\n",
    "    #print(\"\")\n",
    "\n",
    "    # join on key column or columns (original 'set' dtype must be changed to 'list' to fit pandas expected argument for 'merge' method)\n",
    "    df_list[0].merge(df_object, on=list(fieldnames_to_compare), how= 'outer')\n",
    "    print(df_list[0]['Policy'])\n",
    "\n",
    "\n",
    "# add current timestamp to filename for reference\n",
    "current_time = (datetime.datetime.utcnow().strftime('%Y_%m_%d_%H%M%S'))\n",
    "\n",
    "# add 'merged_' to filename startswith and export\n",
    "df_list[0].to_csv(merged_dir + 'merged_' + str(current_time) + \".csv\", index= False)\n",
    "\n",
    "cleanup = True\n",
    "# clean up intermediate data\n",
    "if cleanup == True:\n",
    "    for s in all_shaped_csv:\n",
    "        path = shaped_dir + s\n",
    "        print(f'Removing file {path}')\n",
    "        os.remove(path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for filename in all_source_csv:\n",
    "    source_info = srctime(source_dir + filename)\n",
    "    df = pd.read_csv(source_dir + filename)\n",
    "    for k,v in source_info.items():\n",
    "        print(f'The key is {k}')\n",
    "        print(f'The value is {v}')\n",
    "        df[k] = v\n",
    "    #df.insert(1,columns=source_info.keys(),source_info.values())\n",
    "    #df[source_info.keys()]\n",
    "    #print(source_info)\n",
    "    #print(source_info.keys())\n",
    "    #print(source_info.values())\n",
    "\n",
    "\n",
    "    #print(filename)\n",
    "\n",
    "    #df.colums = fs.patch_columns\n",
    "    #print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": "[REDACTED][REDACTED]/.py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
