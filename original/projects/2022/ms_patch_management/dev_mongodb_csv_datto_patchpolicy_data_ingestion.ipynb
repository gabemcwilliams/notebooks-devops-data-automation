{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1> DattoRMM - Patch Policy - Data Ingestion - MongoDB</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Arguments and Declarations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Selected Columns from Master Device View Export CSV - This is the column mask that will be used to trimp the outer merge on match key column\n",
    "std_columns = [\n",
    "    'Device UID',\n",
    "    'Site Name',\n",
    "    'Site UID',\n",
    "    'Device Hostname',\n",
    "    'Create Date',\n",
    "    'Last Seen',\n",
    "    'Last Audit Date',\n",
    "    'Policy',\n",
    "    'Schedule',\n",
    "    'Last Run',\n",
    "    'Online Duration (hrs)',\n",
    "    'Last Reboot',\n",
    "    'Group',\n",
    "    'Source Modified Date',\n",
    "    'Source Creation Date',\n",
    "    'Source Filename'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# review column filter for any duplicates values or close similarities\n",
    "std_col_ser = pd.Series(std_columns).value_counts()\n",
    "std_col_ser[std_col_ser > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# filename prefix timestamp format\n",
    "time_format = '%Y_%m_%d_%H%M%S'\n",
    "\n",
    "# define key column to join on\n",
    "fieldnames_to_compare = 'Device UID'\n",
    "\n",
    "# identify folder stages so that files are not called twice in the same stage\n",
    "source_dir = 'd:/data_sets/raw/'\n",
    "\n",
    "# Parse Date Data Options\n",
    "date_parser = lambda c: pd.to_datetime(c, errors='coerce')\n",
    "parse_dates =  ['Create Date', 'Last Seen']\n",
    "\n",
    "# NA Values Check\n",
    "na_values = ['Currently Online','null', '(null)']\n",
    "\n",
    "# output of csv with matching key column\n",
    "included_files = {}\n",
    "\n",
    "# output csv of all files that could not be merged\n",
    "excluded_files = {}\n",
    "\n",
    "# Regex Match to group files to be combined on rows rather than merged on columns to prevent dropping rows if there isnt a key column match when files are combined in random order\n",
    "pattern = re.compile(r'^(\\w+)_')\n",
    "\n",
    "# final dataframe before training and visualization\n",
    "df_clean = pd.DataFrame(columns=std_columns)\n",
    "\n",
    "# CSV File Types\n",
    "devices_tab_export_filename = 'DeviceDetailsExport'\n",
    "devices_tab_export_files = []\n",
    "df_devices_list = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "manage_tab_export_filename = 'SystemDeviceSelection'\n",
    "manage_tab_export_files = []\n",
    "df_manage_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Read all files in source_dir and sub directories</h1>\n",
    "    <h3> Filter by '.csv' </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pull all filenames walking through all folders (recursive going down the tree)\n",
    "#all_source_csv = []\n",
    "source_csv_dict = {}\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    for file in files:\n",
    "        if '.csv' in file:\n",
    "            #all_source_csv.append(os.path.join(root,file))\n",
    "            source_csv_dict.update({os.path.join(file):os.path.join(root,file)})\n",
    "\n",
    "# print(all_source_csv)\n",
    "print('All CSV Files found before futher vetting and filtering')\n",
    "print('='*50)\n",
    "for file in source_csv_dict:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Sorting and Excluding Files</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read all csv file columns and create two lists of files:\n",
    "### Those with the chosen merge key column will be kept and the remaining filenames will not be called any further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for each filename - pull source data using source_info module in custom modules folder\n",
    "#for filename in all_source_csv:\n",
    "\n",
    "    # read in dataframe\n",
    "    #df = pd.read_csv(filename)\n",
    "    #print(df['Site Name'].unique())\n",
    "\n",
    "    # for each df review for join key column to be present and add to included_files else add to excluded_files\n",
    "    #print(df.columns)\n",
    "\n",
    "   # compare_keys(df,filename)\n",
    "for k,v in source_csv_dict.items():\n",
    "    df = pd.read_csv(v)\n",
    "    #print(df['Site Name'].unique())\n",
    "    #compare_keys(df,filename)\n",
    "    if fieldnames_to_compare not in df.columns:\n",
    "        print(f'Missing Key: {fieldnames_to_compare} to Join in {filename}')\n",
    "        excluded_files.update({k:v})\n",
    "    else:\n",
    "        included_files.update({k:v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Files with CORRECT join key column:')\n",
    "print('-'*50)\n",
    "for file in included_files:\n",
    "    print(file)\n",
    "print('='*50)\n",
    "\n",
    "print('Files MISSING join key column:')\n",
    "print('-'*50)\n",
    "for file in excluded_files:\n",
    "    print(file)\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parse Accepted CSV's for file discription and store as dictionary key pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'^([a-zA-Z]{0,})(\\_|\\-|''){0,1}([a-zA-Z]{0,})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for k,v in included_files.items():\n",
    "    matches = pattern.search(k)\n",
    "    if matches[1] == devices_tab_export_filename:\n",
    "        print(f'''['{v}'] matches: ['{devices_tab_export_filename}'] on ['{matches[1]}']''')\n",
    "        devices_tab_export_files.append({'filename':v,'groupname':manage_tab_export_filename})\n",
    "    elif matches[3] == manage_tab_export_filename:\n",
    "        print(f'''['{v}'] matches: ['{manage_tab_export_filename}'] on ['{matches[3]}']''')\n",
    "        manage_tab_export_files.append({'filename':v,'groupname':manage_tab_export_filename})\n",
    "\n",
    "\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## For those files that have the key column, set index col and add source info\n",
    "### 1. Add source file data as columns at end of dataframe (record the file creation, modified, and fullpath name)\n",
    "### 2. Set index col = fieldnames_to_compare variable list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def map_source(source_file):\n",
    "    # pull source time from file properties\n",
    "    source_info = srctime(source_file)\n",
    "\n",
    "    # Import CSV\n",
    "    df = pd.read_csv(source_file,index_col=fieldnames_to_compare)\n",
    "\n",
    "    # add source info to new columns k with values v\n",
    "    for k,v in source_info.items():\n",
    "        #print('='*50)\n",
    "        #print(f'key = {k}')\n",
    "        #print(f'value = {v}')\n",
    "        #print('='*50)\n",
    "        df[k] = v\n",
    "        #print(df['Source Creation Date'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Attempt 2 at updating data correctly on import.  Attempting to use iloc for each df row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create blank DataFrame to fill\n",
    "df_master = pd.DataFrame(columns=std_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": "test_details = pd.read_csv('.csv')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": "test_manage = pd.read_csv('.csv')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for row in df_test1.row:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for file in devices_tab_export_files:\n",
    "    #print(file['filename'])\n",
    "    #print(file['groupname'])\n",
    "    df_devices_list.append(map_source(file['filename']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Attempt 1 at updating data correctly on import.  Still having issues with micromanaging file order on join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print(manage_tab_export_files)\n",
    "#print(devices_tab_export_files)\n",
    "for file in devices_tab_export_files:\n",
    "    #print(file['filename'])\n",
    "    #print(file['groupname'])\n",
    "    df_devices_list.append(map_source(file['filename']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for file in manage_tab_export_files:\n",
    "    #print(file['filename'])\n",
    "    #print(file['groupname'])\n",
    "    df_manage_list.append(map_source(file['filename']))\n",
    "\n",
    "\n",
    "#print(df_manage_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices = pd.concat(df_devices_list,axis=0)\n",
    "df_manage = pd.concat(df_manage_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_manage.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_manage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Join, Concat, and Merge</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Those of the same name should be concatonated by row or stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Merge all dataframes (csv's) into an empty dataframe that contains all columns without data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# as a df (dataframe) must be merged on another, we start with filelist with the first element as the df all will be merged into index[0] in list dtype\n",
    "df_clean = df_list[0]\n",
    "\n",
    "for df_object in df_list[1:]:\n",
    "    # join on key column or columns (original 'set' dtype must be changed to 'list' to fit pandas expected argument for 'merge' method)\n",
    "    df_clean.merge(df_object, on=(fieldnames_to_compare), how= 'outer',suffixes=('', '_drop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicate colummns renamed as '_drop' during parse\n",
    "df_clean.drop([col for col in df_clean.columns if 'drop' in col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Trim any columns not in the column standars list 'columns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(df_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_clean.drop([col for col in df_clean.columns if col not in std_columns], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(df_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# review column filter for any duplicates columns and drop one\n",
    "clean_col = dict(df_clean.columns.value_counts())\n",
    "dup_cols = list({k for (k,v) in clean_col.items() if v > 1})\n",
    "if dup_cols:\n",
    "    df_clean.rename(dup_cols,axis=1,inplace=True)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using lists 'parse_dates' as datetime column targets and 'data_parser' as the datetime function to be applied to each value in target columns along each row that will change value type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert dates to datetime\n",
    "# variable 'time_format' stated at declaration\n",
    "df_clean[parse_dates] = df_clean[parse_dates].apply(date_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Add 'Offline 30 days' and 'Extended Reboot' Columns as datetime delta calculations from day this report is run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fill in NaT / NaN data with 0 time so columns can be converted to datetime and datetime methods can be used\n",
    "df_clean[parse_dates].fillna(pd.Timedelta('0 days'),inplace=True)\n",
    "\n",
    "# Filter - Devices Offline 30 Days\n",
    "df_clean['Offline 30 Days'] = df_clean['Last Seen'] > datetime.datetime.now() - pd.to_timedelta(\"30day\")\n",
    "\n",
    "# Filters - Last Reboot Extended Duration and Online without Reboot Extended Duration\n",
    "df_clean['Last Reboot Extended'] = df_clean['Last Reboot'] > datetime.datetime.now() - pd.to_timedelta(\"30day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Apply heatmap to review any NaN or NaT (null) values before they can be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(data = df_clean.isnull(),yticklabels=False,cbar=False,cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Start ML Trials</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Create MS Patching pairplot DataFrame\n",
    "df_patch_pair = df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convert 'Category' columns into numbers to get value relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_patch_status = pd.get_dummies(df_clean['Patch Status'],prefix='d',prefix_sep='_')\n",
    "df_patch_pair.drop('Patch Status',axis=1,inplace=True)\n",
    "df_patch_pair = pd.concat([df_patch_pair,d_patch_status])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_offline_30days = pd.get_dummies(df_clean['Offline 30 Days'],drop_first=True,prefix='d',prefix_sep='_')\n",
    "d_offline_30days.rename(columns={'d_True':'d_Offline_30 Days'},inplace=True)\n",
    "df_patch_pair.drop('Offline 30 Days',axis=1,inplace=True)\n",
    "df_patch_pair = pd.concat([df_patch_pair,d_offline_30days])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_last_reboot_ext = pd.get_dummies(df_clean['Last Reboot Extended'],drop_first=True,prefix='d',prefix_sep='_')\n",
    "d_last_reboot_ext.rename(columns={'d_True':'d_Last Reboot Extended'},inplace=True)\n",
    "df_patch_pair.drop('Last Reboot Extended',axis=1,inplace=True)\n",
    "df_patch_pair = pd.concat([df_patch_pair,d_last_reboot_ext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_av_status = pd.get_dummies(df_clean['Antivirus Status'],drop_first=True,prefix='d',prefix_sep='_')\n",
    "d_av_status.rename(columns={'d_Running & up-to-date':'d_AV Status Ok'},inplace=True)\n",
    "df_patch_pair.drop('Antivirus Status',axis=1,inplace=True)\n",
    "df_patch_pair = pd.concat([df_patch_pair,d_av_status])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_reboot_required = pd.get_dummies(df_clean['Reboot required'],drop_first=True,prefix='d',prefix_sep='_')\n",
    "d_reboot_required.rename(columns={'d_True':'d_Reboot required'},inplace=True)\n",
    "df_patch_pair.drop('Reboot required',axis=1,inplace=True)\n",
    "df_patch_pair = pd.concat([df_patch_pair,d_reboot_required])\n",
    "d_reboot_required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=df_patch_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for col in df_patch_pair.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(30,15))\n",
    "sns.lineplot(data=df_patch_pair,x='Last Reboot',y='Patches Approved Pending',lw=.5)\n",
    "plt.savefig('.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_patch_pair['Online Duration (hrs)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_patch_pair['Site Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(30,15))\n",
    "sns.barplot(data=df_patch_pair,x='Site Name',y='Online Duration (hrs)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# store filename info as dictionary\n",
    "var_dict = {}\n",
    "i = 0\n",
    "df_list = []\n",
    "df_names =[]\n",
    "\n",
    "\n",
    "# create variable dictionary before import iteration\n",
    "for filename in all_shaped_csv:\n",
    "    var_dict.update({('df' + str(i)): (shaped_dir + filename)})\n",
    "    i = i + 1\n",
    "\n",
    "# for filename = key(k) import into pandas and append resulting dataframe to df list as element\n",
    "for k, v in var_dict.items():\n",
    "    k = pd.read_csv(v)\n",
    "    df_names.append(v)\n",
    "    df_list.append(k)\n",
    "\n",
    "#print(df_names)\n",
    "\n",
    "# as a df (dataframe) must be merged on another, we start with filelist with the first element as the df all will be merged into index[0] in list dtype\n",
    "for df_object in df_list[1:]:\n",
    "    #print(df_object)\n",
    "    #print(\"\")\n",
    "\n",
    "    # join on key column or columns (original 'set' dtype must be changed to 'list' to fit pandas expected argument for 'merge' method)\n",
    "    df_list[0].merge(df_object, on=list(fieldnames_to_compare), how= 'outer')\n",
    "    print(df_list[0]['Policy'])\n",
    "\n",
    "\n",
    "# add current timestamp to filename for reference\n",
    "current_time = (datetime.datetime.utcnow().strftime('%Y_%m_%d_%H%M%S'))\n",
    "\n",
    "# add 'merged_' to filename startswith and export\n",
    "df_list[0].to_csv(merged_dir + 'merged_' + str(current_time) + \".csv\", index= False)\n",
    "\n",
    "cleanup = True\n",
    "# clean up intermediate data\n",
    "if cleanup == True:\n",
    "    for s in all_shaped_csv:\n",
    "        path = shaped_dir + s\n",
    "        print(f'Removing file {path}')\n",
    "        os.remove(path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for filename in all_source_csv:\n",
    "    source_info = srctime(source_dir + filename)\n",
    "    df = pd.read_csv(source_dir + filename)\n",
    "    for k,v in source_info.items():\n",
    "        print(f'The key is {k}')\n",
    "        print(f'The value is {v}')\n",
    "        df[k] = v\n",
    "    #df.insert(1,columns=source_info.keys(),source_info.values())\n",
    "    #df[source_info.keys()]\n",
    "    #print(source_info)\n",
    "    #print(source_info.keys())\n",
    "    #print(source_info.values())\n",
    "\n",
    "\n",
    "    #print(filename)\n",
    "\n",
    "    #df.colums = fs.patch_columns\n",
    "    #print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import json\n",
    "\n",
    "\n",
    "# add current timestamp to filename for reference\n",
    "current_time = (dt.datetime.utcnow().strftime('%Y_%m_%d_%H%M%S'))\n",
    "\n",
    "# export folder will contain all csv exported DataFrames for Ticket Creation\n",
    "export_folder = 'd:/exports/'\n",
    "\n",
    "# import configparser for env secrets\n",
    "from configparser import ConfigParser\n",
    "config = ConfigParser()\n",
    "config.read('d:/git/example_infrastructure_data_dev/config/env.ini')\n",
    "import requests\n",
    "from requests.structures import CaseInsensitiveDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DattoRMM - Pull JSON from API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create Auth Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create DataFrame via API Call Iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DattoRMM DataFrame - import and assign secrets from env.ini\n",
    "base_uri = config['dattormm']['base_uri']\n",
    "api_key = config['dattormm']['api_key']\n",
    "api_secret = config['dattormm']['api_secret']\n",
    "# call token api url\n",
    "token_uri = config['dattormm']['token_uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# construct header\n",
    "headers = CaseInsensitiveDict()\n",
    "headers['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "\n",
    "# construct req body\n",
    "data = CaseInsensitiveDict()\n",
    "data['grant_type'] = 'password'\n",
    "data['username'] = api_key\n",
    "data['password'] = api_secret\n",
    "\n",
    "# request content response\n",
    "resp = requests.post(token_uri, headers=headers, data=data, auth=('public-client', 'public'))\n",
    "content = resp.content.decode('utf-8')\n",
    "c_dict = json.loads(content)\n",
    "\n",
    "access_token = c_dict['access_token']\n",
    "## Create Devices DataFrame\n",
    "# request content response\n",
    "request_url = f'{base_uri}/account/devices'\n",
    "\n",
    "# construct header\n",
    "headers = CaseInsensitiveDict()\n",
    "headers['Authorization'] = f'Bearer {access_token}'\n",
    "headers['Content-Type'] = 'application/json'\n",
    "\n",
    "# construct req body\n",
    "data = ''\n",
    "\n",
    "print(f'Request URL: {request_url}')\n",
    "\n",
    "resp = requests.get(request_url, headers=headers, data=data)\n",
    "content = resp.content.decode('utf-8')\n",
    "c_dict = json.loads(content)\n",
    "print(c_dict['pageDetails']['nextPageUrl'])\n",
    "\n",
    "# iterate and combine remaining pages\n",
    "df_devices = pd.DataFrame(c_dict['devices'])\n",
    "while c_dict['pageDetails']['nextPageUrl']:\n",
    "    next_page = c_dict['pageDetails']['nextPageUrl']\n",
    "    resp = requests.get(next_page, headers=headers, data=data)\n",
    "    content = resp.content.decode('utf-8')\n",
    "    c_dict = json.loads(content)\n",
    "    print(c_dict['pageDetails']['nextPageUrl'])\n",
    "    df_current_page = pd.DataFrame(c_dict['devices'])\n",
    "    df_devices = pd.concat([df_devices, df_current_page], ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create New Columns from Dictionary Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Set Index to device UID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices.set_index('uid',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Type | Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def device_category(device):\n",
    "    if device is None:\n",
    "        return None\n",
    "    else:\n",
    "        return device['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def device_type(device):\n",
    "    if device is None:\n",
    "        return None\n",
    "    else:\n",
    "        return device['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices['category'] = df_devices['deviceType'].apply(device_category)\n",
    "df_devices['type'] = df_devices['deviceType'].apply(device_type)\n",
    "\n",
    "# Rename 'type' values to split devices into (2) : 'computer' or 'server'\n",
    "#df_devices['type'].replace({'Desktop':'computer','Laptop':'computer','Server':'server'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices.drop(columns='deviceType',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Patch Management Breakdown\n",
    " patchStatus | patchesApprovedPending | patchesNotApproved | patchesInstalled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# patchStatus\n",
    "def patch_status(patch_management):\n",
    "    return patch_management['patchStatus']\n",
    "\n",
    "df_devices['patchStatus'] = df_devices['patchManagement'].apply(patch_status)\n",
    "\n",
    "# patchesApprovedPending\n",
    "def patches_approved_pending(patch_management):\n",
    "    return patch_management['patchesApprovedPending']\n",
    "\n",
    "df_devices['patchesApprovedPending'] = df_devices['patchManagement'].apply(patches_approved_pending)\n",
    "\n",
    "# patchesNotApproved\n",
    "def patches_not_approved(patch_management):\n",
    "    return patch_management['patchesNotApproved']\n",
    "\n",
    "df_devices['patchesNotApproved'] = df_devices['patchManagement'].apply(patches_not_approved)\n",
    "\n",
    "# patchesInstalled\n",
    "def patches_installed(patch_management):\n",
    "    return patch_management['patchesInstalled']\n",
    "\n",
    "df_devices['patchesInstalled'] = df_devices['patchManagement'].apply(patches_installed)\n",
    "\n",
    "\n",
    "# drop patchManagement {inplace=True}\n",
    "df_devices.drop('patchManagement',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Drop 'antivirus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices.drop('antivirus',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create Time Columns and Timedate Shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Add Timezone Column from UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Timezone\n",
    "def local_timezone(udf):\n",
    "    return udf['udf10']\n",
    "\n",
    "df_devices['localTimezone'] = df_devices['udf'].apply(local_timezone)\n",
    "\n",
    "# drop udf {inplace=True}\n",
    "df_devices.drop('udf',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create Date Correlation Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# all date columns\n",
    "parse_dates =  ['lastAuditDate','lastSeen','lastReboot','creationDate',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Convert Epoch to UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices['lastAuditDate'] = pd.to_datetime(df_devices['lastAuditDate'],unit='ms',errors='coerce')\n",
    "#df_devices['lastAuditDate'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices['lastSeen'] = pd.to_datetime(df_devices['lastSeen'],unit='ms',errors='coerce')\n",
    "#df_devices['lastSeen'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices['creationDate'] = pd.to_datetime(df_devices['creationDate'],unit='ms',errors='coerce')\n",
    "#df_devices['creationDate'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices['lastReboot'] = pd.to_datetime(df_devices['lastReboot'],unit='ms',errors='coerce')\n",
    "#df_devices['lastReboot'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define and apply functions to create correlation columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def no_audit_7_days(last_audit):\n",
    "    if last_audit < dt.datetime.now() - dt.timedelta(days=7):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def offline_30_days(last_seen):\n",
    "    if last_seen < dt.datetime.now() - dt.timedelta(days=60):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def no_reboot_60_days(last_reboot):\n",
    "    if last_reboot < dt.datetime.now() - dt.timedelta(days=60):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Column - Devices Last Audit > 7 days\n",
    "df_devices['noAudit7Days'] = df_devices['lastAuditDate'].apply(no_audit_7_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Column - Devices Offline 30 Days\n",
    "df_devices['offline30Days'] = df_devices['lastSeen'].apply(offline_30_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Column - Last Reboot Extended Duration and Online without Reboot Extended Duration\n",
    "df_devices['noReboot60Days'] = df_devices['lastReboot'].apply(no_reboot_60_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create DF copy for reference\n",
    "df_raw_data = df_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## DattoRMM DataFrame Data Standardization Shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Hostname to_upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices['hostname'] = df_devices['hostname'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Replace Values {'TRUE':1,'FALSE':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices.replace({True:1,False:0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "convert_to_int = df_devices.dtypes[(df_devices.dtypes == 'float') | (df_devices.dtypes == 'bool') | (df_devices.dtypes == 'uint8')].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices[convert_to_int] = df_devices[convert_to_int].astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Drop Columns with no Data (NaN, 0, or None as only Value on all rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_devices.drop(['lastLoggedInUser','warrantyDate'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Add 'patchStatus' Dummy Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_patch_status = pd.get_dummies(df_devices['patchStatus'],prefix='patchStatus')\n",
    "df_patch_status.drop('patchStatus_NoPolicy',axis=1, inplace=True)\n",
    "df_devices = df_devices.join(df_patch_status)\n",
    "df_devices.drop('patchStatus',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "username = config['mongodb']['username']\n",
    "password = config['mongodb']['password']\n",
    "connection_ip = config['mongodb']['connection_ip']\n",
    "database = 'seed_data'\n",
    "\n",
    "# import bson for object encoding\n",
    "import bson\n",
    "\n",
    "# Provide the mongodb atlas url to connect python to mongodb using pymongo\n",
    "#CONNECTION_STRING = f\"mongodb://{username}:{password}@{connection_ip}/{database}\"\n",
    "CONNECTION_STRING = 'mongodb://localhost:27017'\n",
    "\n",
    "\n",
    "# Create a connection using MongoClient. You can import MongoClient or use pymongo.MongoClient\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient(CONNECTION_STRING)\n",
    "\n",
    "\n",
    "db = client['datto_rmm']\n",
    "collection = db['seed_data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_ingest = df_devices.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dt_list = list(df_ingest.dtypes[df_ingest.dtypes == 'datetime64[ns]'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_ingest.fillna(pd.NA,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "entries = []\n",
    "cols = enumerate(list(df_ingest.columns))\n",
    "\n",
    "for index, row in df_ingest.iterrows():\n",
    "    current_row = row.to_dict()\n",
    "    new_entry = {}\n",
    "    try:\n",
    "        try:\n",
    "            new_entry['onlineData'] = {'legend':['onlineStatus','lastSeen'],'data':[current_row['online'],current_row['lastSeen']]}\n",
    "        except:\n",
    "            print(f'Unable to create column: \"onlineData\"')\n",
    "        try:\n",
    "            if current_row['suspended'] == 1:\n",
    "                new_entry['isSuspended'] = {'legend':['suspendedStatus','lastSeen'],'data':[current_row['suspended'],current_row['lastSeen']]}\n",
    "        except:\n",
    "            print(f'Unable to create column: \"isSuspended\"')\n",
    "            print(current_row['suspended'])\n",
    "        try:\n",
    "            if current_row['deleted'] == 1:\n",
    "                new_entry['isDeleted'] = {'legend':['deletedStatus','lastSeen'],'data':[current_row['deleted'],current_row['lastSeen']]}\n",
    "        except:\n",
    "            print(f'Unable to create column: \"isDeleted\"')\n",
    "            print(current_row['deleted'])\n",
    "        try:\n",
    "            if current_row['rebootRequired'] == 1:\n",
    "                new_entry['rebootRequired'] = {'legend':['rebootRequired','lastReboot'],'data':[current_row['rebootRequired'],current_row['lastReboot']]}\n",
    "        except:\n",
    "            print(f'Unable to create column: \"rebootRequired\"')\n",
    "            print(current_row['rebootRequired'])\n",
    "        for k,v in current_row.items():\n",
    "            if pd.isna(v) == True:\n",
    "                continue\n",
    "            elif (k == 'online') | (k == 'lastSeen') | (k == 'suspended') | (k == 'rebootRequired') | (k == 'lastReboot') |(k == 'deleted'):\n",
    "                continue\n",
    "            else:\n",
    "                new_entry[k] = v\n",
    "\n",
    "        entries.append(new_entry)\n",
    "    except:\n",
    "        print(current_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "entries[55]['hostname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "collection.insert_one(entries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result = collection.find_one({'uid':entries[0]['uid']})\n",
    "result['_id']\n",
    "match_id = result['_id']\n",
    "k = 'onlineStatus'\n",
    "v = {\"legend\":'test'}\n",
    "collection.update_one({'_id':match_id},{'$set': {f\"{ str(k)  + '.legend'}\" : v['legend'] }},upsert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for entry in entries:\n",
    "    try:\n",
    "        result = collection.find_one({'uid':entry['uid']})\n",
    "        print(f\"_id found: {result['_id']}\")\n",
    "        match_id = result['_id']\n",
    "        for k,v in entry.items():\n",
    "            try:\n",
    "                #print(k,v)\n",
    "                if k in  ['onlineData','isSuspended','isDeleted','rebootRequired']:\n",
    "                    try:\n",
    "                        #print({f\"{ str(k)  + '.legend'}\": v['legend']})\n",
    "                        #collection.insert_one({'_id':m_id}, {f\"{str(k)}\":v['legend']})\n",
    "                        collection.update_one({'_id':match_id},{'$set': {f\"{ str(k)  + '.legend'}\" : v['legend'] }},upsert=False)\n",
    "                        collection.update_one({'_id':match_id},{'$push': {f\"{ str(k)  + '.data'}\" : {\"$each\":  v['data'] }}},upsert=True)\n",
    "                        #print(f\"Successfully updated {k} for {entry['hostname']}\")\n",
    "                    except Exception as error:\n",
    "                        print(error)\n",
    "                else:\n",
    "                    try:\n",
    "                        collection.update_one({'_id':match_id}, {'$set':{f\"{str(k)}\":v}},upsert=True)\n",
    "                        #print(f\"Successfully inserted new {k} for {entries[0]['hostname']}\")\n",
    "                    except Exception as error:\n",
    "                        #print(f\"Unable to insert new {k} for {entries[0]['hostname']}\")\n",
    "                        print(error)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            print(f\"_id not found: attempting new entry for {entry['hostname']} with uid: {entry['uid']}\")\n",
    "            collection.insert_one(entry)\n",
    "        except:\n",
    "            print(e)\n",
    "\n",
    "print('*'*120)\n",
    "print('End of Import!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": "[REDACTED][REDACTED]/.py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
