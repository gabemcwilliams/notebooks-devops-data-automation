{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed08a884afde0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CentraStage Agent Log Parser\n",
    "----------------------------\n",
    "\n",
    "Parses and analyzes log files from CentraStage/Datto RMM agents. Focuses on extracting DNS activity\n",
    "and error-related messages to support monitoring, forensic investigations, or anomaly detection.\n",
    "\n",
    "Main Capabilities:\n",
    "------------------\n",
    "- Recursively searches specified log directories for `.log` files\n",
    "- Parses structured log lines using pipe-delimited format\n",
    "- Extracts and expands JSON-like content from log messages\n",
    "- Filters for DNS calls to known Concord/CentraStage endpoints\n",
    "- Extracts errors and exception messages from all logs\n",
    "- Outputs:\n",
    "    - Parsed DNS resolution entries (`df_dns_info`)\n",
    "    - Parsed error entries (`df_errors_info`)\n",
    "    - Value counts for log activity rates by message type\n",
    "\n",
    "Use Cases:\n",
    "----------\n",
    "- Analyzing agent communication with RMM control servers\n",
    "- Diagnosing error behavior across logs\n",
    "- Calculating log traffic volume over time\n",
    "- Investigating potential downtime, failures, or network changes\n",
    "\n",
    "Limitations:\n",
    "------------\n",
    "- Assumes a consistent pipe-delimited log format\n",
    "- DNS and error filters are hardcoded (could be extended)\n",
    "- Some methods lack defensive error checks on malformed logs\n",
    "\n",
    "Author: Gabe McWilliams\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5f4c0a9025c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseLogs:\n",
    "    def __init__(self,\n",
    "                 logs_dirs: list,\n",
    "                 dns_destinations_list=None\n",
    "                 ) -> None:\n",
    "\n",
    "        # establish logs dir\n",
    "        if dns_destinations_list is None:\n",
    "            dns_destinations_list = [\n",
    "                'concord-monitoring.centrastage.net',\n",
    "                'update-concord.centrastage.net',\n",
    "                'concord-agent-comms.centrastage.net',\n",
    "                'concord-agent.centrastage.net',\n",
    "                'concord-agent-notifications.centrastage.net',\n",
    "                '01concordcc.centrastage.net',\n",
    "                'concord-frontend-api.centrastage.net',\n",
    "                'concord-realtime.centrastage.net',\n",
    "                'concord.centrastage.net',\n",
    "                'concordcc.centrastage.net',\n",
    "                'concordws.centrastage.net',\n",
    "                'update-concord-proxy.centrastage.net'\n",
    "            ]\n",
    "        self.__logs_dirs = logs_dirs\n",
    "\n",
    "        # establish dns destinations to parse for\n",
    "        self.__dns_destinations_list = dns_destinations_list\n",
    "\n",
    "        # fetch files ending in 'log' and append to list\n",
    "        self.__log_files_list = self.__logs_list__()\n",
    "\n",
    "        # return dataframes from log parsing\n",
    "        self.__logs_dns_info = self.__parse_logs_dns_info__()\n",
    "\n",
    "        self.__logs_errors_info = self.__parse_logs_errors_info__()\n",
    "\n",
    "    @staticmethod\n",
    "    def __split_log_line__(line: str) -> dict:\n",
    "        result = line.split('|')\n",
    "\n",
    "        return {\n",
    "            'agent_ver': result[0],\n",
    "            'timestamp': result[1],\n",
    "            'message_lvl': result[2],\n",
    "            'action': result[3],\n",
    "            'module_info': result[4]\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def logs_dirs(self) -> list:\n",
    "        return self.__log_dirs\n",
    "\n",
    "    @property\n",
    "    def dns_destinations(self) -> list:\n",
    "        return self.__dns_destinations_list\n",
    "\n",
    "    @property\n",
    "    def log_files(self) -> list:\n",
    "        return self.__log_files\n",
    "\n",
    "    def __logs_list__(self) -> list:\n",
    "        log_files_list = []\n",
    "\n",
    "        for log_loc in self.__logs_dirs:\n",
    "            for root, dirs, files in os.walk(log_loc):\n",
    "                for file in files:\n",
    "                    if 'log' in file:\n",
    "                        log_files_list.append(os.path.join(root, file))\n",
    "\n",
    "        return log_files_list\n",
    "\n",
    "    @property\n",
    "    def df_dns_info(self):\n",
    "        df = pd.DataFrame(self.__logs_dns_info)\n",
    "\n",
    "        df[['agent_ver', 'timestamp', 'message_lvl', 'action', 'module_info']] = df.apply(\n",
    "            lambda x: self.__split_log_line__(x['line']), axis=1, result_type='expand')\n",
    "        for index, row in df[:].iterrows():\n",
    "\n",
    "            pattern = r'\\\"([^\"]+)\\\":\\s*\\\"([^\"]+)\\\"'\n",
    "            matches = re.findall(pattern, row['module_info'])\n",
    "\n",
    "            for match in matches:\n",
    "                df.loc[index, match[0]] = match[1]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def __parse_logs_dns_info__(self) -> dict:\n",
    "        dns_info_list = []\n",
    "        log_lines_parsed = 0\n",
    "\n",
    "        for file in self.__log_files_list[:]:\n",
    "            with open(file, 'r') as f:\n",
    "                logs_info_dict = {}\n",
    "                logs = f.readlines()\n",
    "                for index, line in enumerate(logs):\n",
    "                    log_lines_parsed = log_lines_parsed + 1\n",
    "\n",
    "                    for dns_dest in dns_destinations_list:\n",
    "                        if dns_dest in line:\n",
    "                            # print(f'Found {dns_dest} on line: {line}')\n",
    "                            logs_info_dict['file'] = file\n",
    "                            logs_info_dict['index'] = index\n",
    "                            logs_info_dict['line'] = line\n",
    "                            dns_info_list.append(logs_info_dict)\n",
    "\n",
    "        print(f'Number of log files parsed: {len(log_files_list)}')\n",
    "        print(f'Number of log lines parse: {log_lines_parsed}')\n",
    "\n",
    "        return dns_info_list\n",
    "\n",
    "    @property\n",
    "    def df_errors_info(self) -> pd.DataFrame:\n",
    "        df[['agent_ver', 'timestamp', 'message_lvl', 'action', 'module_info']] = df.apply(\n",
    "            lambda x: self.__split_log_line__(x['line']), axis=1, result_type='expand')\n",
    "        for index, row in df[:].iterrows():\n",
    "\n",
    "            pattern = r'\\\"([^\"]+)\\\":\\s*\\\"([^\"]+)\\\"'\n",
    "            matches = re.findall(pattern, row['module_info'])\n",
    "\n",
    "            for match in matches:\n",
    "                df.loc[index, match[0]] = match[1]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def __parse_logs_errors_info__(self) -> dict:\n",
    "\n",
    "        errors_info_list = []\n",
    "        log_lines_parsed = 0\n",
    "\n",
    "        for file in self.__log_files_list[:]:\n",
    "            with open(file, 'r') as f:\n",
    "                logs_info_dict = {}\n",
    "                logs = f.readlines()\n",
    "                for index, line in enumerate(logs):\n",
    "                    log_lines_parsed = log_lines_parsed + 1\n",
    "\n",
    "                    if ('ERROR' in line) | ('exception' in line):\n",
    "                        logs_info_dict['file'] = file\n",
    "                        logs_info_dict['index'] = index\n",
    "                        logs_info_dict['line'] = line\n",
    "                        errors_info_list.append(logs_info_dict)\n",
    "\n",
    "        return errors_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22594faf80df2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logs_dirs = [\n",
    "#     \"C:\\ProgramData\\CentraStage\\AEMAgent\\DataLog\".replace(\"\\\\\", \"/\"),\n",
    "#     \"C:\\Program Files (x86)\\CentraStage\".replace(\"\\\\\", \"/\")\n",
    "# ]\n",
    "\n",
    "logs_dirs = [\n",
    "    \"D:/example\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b183acd5bfcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ParseLogs(logs_dirs=logs_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be1c645c66748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dns_info = parser.df_dns_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b093fc91caa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dns_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc836056711bbabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors_log_info = parser.df_errors_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625fb25fb63eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors_log_info.Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5ec5af075a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "max = pd.to_datetime(df_file_log_info[\"timestamp\"].max())\n",
    "min = pd.to_datetime(df_file_log_info[\"timestamp\"].min())\n",
    "\n",
    "print(f\"The first time entry was: {min}\\nThe last time entry as: {max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4effc35a1b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_duration = round((pd.Timedelta(max - min).total_seconds()) / 3600)\n",
    "print(f\"The dataset time frame was {hours_duration} hours\")\n",
    "minutes_duration = round((pd.Timedelta(max - min).total_seconds()) / 60)\n",
    "print(f\"The dataset time frame was {minutes_duration} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb377a5935fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_centrastage_value_counts = df_file_log_info[\"module_info\"].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d489979085149a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_centrastage_value_counts['counts_per_hour'] = df_centrastage_value_counts['count'].apply(\n",
    "    lambda x: x / hours_duration)\n",
    "df_centrastage_value_counts['counts_per_minute'] = df_centrastage_value_counts['count'].apply(\n",
    "    lambda x: x / minutes_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff81b7e5956c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_centrastage_value_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a60af0df123b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_centrastage_value_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": "[REDACTED][REDACTED]/.py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
